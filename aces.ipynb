{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a06ddaaa-b2d7-4da8-a0ef-9c2894042ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import GaussianBlur\n",
    "\n",
    "from brightness import dino\n",
    "from brightness.tone_mappers.aces import aces\n",
    "\n",
    "def assert_no_nans(x):\n",
    "    assert torch.isnan(x.view(-1)).sum().item() == 0\n",
    "\n",
    "def tensor_rgb_to_xyz(image):\n",
    "    rgb_to_xyz_matrix = torch.Tensor(\n",
    "        [\n",
    "            [0.4124564, 0.3575761, 0.1804375],\n",
    "            [0.2126729, 0.7151522, 0.0721750],\n",
    "            [0.0193339, 0.1191920, 0.9503041],\n",
    "        ]\n",
    "    ).to(image.device)\n",
    "    xyz = image @ rgb_to_xyz_matrix.T\n",
    "    return xyz[..., 0], xyz[..., 1], xyz[..., 2]\n",
    "\n",
    "\n",
    "def pytorch_gaussian_blur(image, sigma):\n",
    "    blur = GaussianBlur(kernel_size=int(2 * sigma + 1) | 1, sigma=sigma)\n",
    "    if len(image.size()) < 3:\n",
    "        image = image.unsqueeze(0)\n",
    "    return blur(image).squeeze()\n",
    "\n",
    "\n",
    "def pytorch_dn_brightness_model(\n",
    "    L: torch.Tensor,\n",
    "    cs_ratio: float = 2.0,\n",
    "    min_scale: float = 1.0,\n",
    "    gamma: float = 2.2,\n",
    "    w: float = 0.85,\n",
    "    a: float = 1.0,\n",
    "    b: float = 1.0,\n",
    "    c: float = 1.0,\n",
    "    d: float = 1.0,\n",
    "    scale_normalized_constants: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    L = torch.pow(L, (1.0 / gamma))\n",
    "    \n",
    "    max_scale = min(*L.size())\n",
    "    scale = max_scale / cs_ratio\n",
    "    scales = []\n",
    "    while scale >= min_scale:\n",
    "        scales.insert(0, scale)\n",
    "        scale /= cs_ratio\n",
    "    weights = [w**i for i in range(len(scales))]\n",
    "    # Initialize weighted_sum as a 2D array\n",
    "    weighted_sum = torch.zeros(L.size()[:2]).to(L.device)\n",
    "    # Compute ratios and weighted sum using only two blurred images at a time\n",
    "    center_response = pytorch_gaussian_blur(L, scales[0])\n",
    "    for i in range(1, len(scales)):\n",
    "        surround_response = pytorch_gaussian_blur(L, scales[i])\n",
    "        assert center_response.ndim == 2\n",
    "        assert surround_response.ndim == 2\n",
    "        _b = b\n",
    "        _d = d\n",
    "        if scale_normalized_constants:\n",
    "            _b /= scales[i] ** 2\n",
    "            _d /= scales[i] ** 2\n",
    "        weighted_sum += weights[i - 1] * (\n",
    "            (a * center_response + _b) / (c * surround_response + _d) - _b / _d\n",
    "        )\n",
    "        center_response = surround_response\n",
    "    assert_no_nans(weighted_sum)\n",
    "    return weighted_sum\n",
    "\n",
    "\n",
    "class ACESModel(nn.Module):\n",
    "    W = 0.9\n",
    "    A = 1\n",
    "    B = 1\n",
    "    C = 1\n",
    "    D = 1\n",
    "    SCALE_NORMALIZED = True\n",
    "    KEY = 0.18\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ACESModel, self).__init__()\n",
    "        # a, b, c, d, e\n",
    "        self.weights = nn.Parameter(torch.Tensor([2.51, 0.03, 2.43, 0.59, 0.14]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply ACES tone mapper\n",
    "        x = (self.KEY / (torch.mean(x**2)**(1/2))) * x\n",
    "        assert_no_nans(x)\n",
    "        a, b, c, d, e = self.weights\n",
    "        tonemapped_x = torch.squeeze((x * (a*x + b)) / (x * (c*x + d) + e))\n",
    "        # Clip results to (0, 1)\n",
    "        zeros = torch.zeros(tonemapped_x.size()).to(x.device)\n",
    "        tonemapped_x = torch.maximum(tonemapped_x, zeros)\n",
    "        ones = torch.full(tonemapped_x.size(), 1).to(x.device)\n",
    "        tonemapped_x = torch.minimum(tonemapped_x, ones)\n",
    "        # Compute brightness response\n",
    "        X, Y, Z = tensor_rgb_to_xyz(tonemapped_x)\n",
    "        L, _, _ = dino.xyz_to_lxy(X, Y, Z)\n",
    "        brightness_response = pytorch_dn_brightness_model(\n",
    "            L,\n",
    "            w=self.W,\n",
    "            a=self.A,\n",
    "            b=self.B,\n",
    "            c=self.C,\n",
    "            d=self.D,\n",
    "            scale_normalized_constants=self.SCALE_NORMALIZED,\n",
    "        )\n",
    "        return brightness_response.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc07df27-7c6c-4d05-8d79-379bf657d6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70eb8550a28f4588a9a3fdfc322e3247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generating training set:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training/brightnesses/CadikNight.npy already exists, skipping...\n",
      "training/brightnesses/CadikDesk.npy already exists, skipping...\n",
      "training/brightnesses/CadikWindow.npy already exists, skipping...\n",
      "training/brightnesses/LuxoDoubleChecker.npy already exists, skipping...\n",
      "training/brightnesses/HDRMark.npy already exists, skipping...\n",
      "training/brightnesses/GeneralGrant.npy already exists, skipping...\n",
      "training/brightnesses/DelicateArch.npy already exists, skipping...\n",
      "training/brightnesses/507.npy already exists, skipping...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def file_to_brightness(file_path):\n",
    "    image = dino.read_image(file_path)\n",
    "    X, Y, Z = dino.rgb_to_xyz(image)\n",
    "    L, _, _ = dino.xyz_to_lxy(X, Y, Z)\n",
    "    return dino.dn_brightness_model(\n",
    "        L,\n",
    "        w=ACESModel.W,\n",
    "        a=ACESModel.A,\n",
    "        b=ACESModel.B,\n",
    "        c=ACESModel.C,\n",
    "        d=ACESModel.D,\n",
    "        scale_normalized_constants=ACESModel.SCALE_NORMALIZED,\n",
    "    )\n",
    "\n",
    "\n",
    "# Generate brightness response images for all inputs\n",
    "data_dir = Path(\"training\")\n",
    "for file in tqdm(list((data_dir / \"images\").iterdir()), desc=\"generating training set\"):\n",
    "    try:\n",
    "        export_path = str(data_dir / \"brightnesses\" / file.stem) + \".npy\"\n",
    "        if Path(export_path).exists():\n",
    "            print(f\"{export_path} already exists, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        brightness = file_to_brightness(str(file))\n",
    "        print(f\"Exporting {export_path}\")\n",
    "        np.save(export_path, brightness)\n",
    "    except Exception as e:\n",
    "        print(f\"File {file} could not be processed due to {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cfe97b8-aaa5-4ed9-9577-7227dd265e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda\n"
     ]
    }
   ],
   "source": [
    "class BrightnessDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, input_dir, target_dir, max_width=1000, transform=None, target_transform=None):\n",
    "        self.input_file_paths = sorted(f for f in Path(input_dir).iterdir() if f.is_file())\n",
    "        self.target_file_paths = sorted(f for f in Path(target_dir).iterdir() if f.is_file())\n",
    "\n",
    "        assert all(\n",
    "            self.input_file_paths[i].stem == self.target_file_paths[i].stem\n",
    "            for i in range(len(self.input_file_paths))\n",
    "        )\n",
    "\n",
    "        self.max_width = max_width\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_image = dino.read_image(str(self.input_file_paths[idx]))\n",
    "        input_image = dino.resize_image(\n",
    "            input_image, resize_width=self.max_width\n",
    "        )\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            \n",
    "        brightness_response = np.load(self.target_file_paths[idx])\n",
    "        brightness_response = dino.resize_image(\n",
    "            brightness_response, resize_width=self.max_width\n",
    "        )\n",
    "        if self.target_transform:\n",
    "            brightness_response = self.target_transform(brightness_response)\n",
    "\n",
    "        return input_image, brightness_response\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device is {device}\")\n",
    "model = ACESModel().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "train_set = BrightnessDataset(\"training/images\", \"training/brightnesses\")\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90495787-a115-4950-8c57-ce90dadbede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = Path(\"training/checkpoints\")\n",
    "\n",
    "def checkpoint_file_path(epoch):\n",
    "    return str(CHECKPOINT_PATH / f\"aces-epoch{epoch}.pickle\")\n",
    "\n",
    "start_epoch = 0\n",
    "if start_epoch == 0:\n",
    "    checkpoint = dict(\n",
    "        state_dict={},\n",
    "        train_losses=[],\n",
    "        train_errors=[],\n",
    "    )\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint_file_path(start_epoch), weights_only=False)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    print(f\"Loaded model from checkpoint at epoch {start_epoch}.\")\n",
    "\n",
    "e = start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e858033-db67-4e41-832d-dff2f1e1185c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92af41ed61bc42cca311143a93524940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m sample \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m brightness \u001b[38;5;241m=\u001b[39m brightness\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(prediction, brightness)\n\u001b[1;32m     10\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/Documents/GitHub/divnorm-brightness/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/divnorm-brightness/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 101\u001b[0m, in \u001b[0;36mACESModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     99\u001b[0m X, Y, Z \u001b[38;5;241m=\u001b[39m tensor_rgb_to_xyz(tonemapped_x)\n\u001b[1;32m    100\u001b[0m L, _, _ \u001b[38;5;241m=\u001b[39m dino\u001b[38;5;241m.\u001b[39mxyz_to_lxy(X, Y, Z)\n\u001b[0;32m--> 101\u001b[0m brightness_response \u001b[38;5;241m=\u001b[39m \u001b[43mpytorch_dn_brightness_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale_normalized_constants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSCALE_NORMALIZED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m brightness_response\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 69\u001b[0m, in \u001b[0;36mpytorch_dn_brightness_model\u001b[0;34m(L, cs_ratio, min_scale, gamma, w, a, b, c, d, scale_normalized_constants)\u001b[0m\n\u001b[1;32m     65\u001b[0m     weighted_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weights[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m     66\u001b[0m         (a \u001b[38;5;241m*\u001b[39m center_response \u001b[38;5;241m+\u001b[39m _b) \u001b[38;5;241m/\u001b[39m (c \u001b[38;5;241m*\u001b[39m surround_response \u001b[38;5;241m+\u001b[39m _d) \u001b[38;5;241m-\u001b[39m _b \u001b[38;5;241m/\u001b[39m _d\n\u001b[1;32m     67\u001b[0m     )\n\u001b[1;32m     68\u001b[0m     center_response \u001b[38;5;241m=\u001b[39m surround_response\n\u001b[0;32m---> 69\u001b[0m \u001b[43massert_no_nans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweighted_sum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weighted_sum\n",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m, in \u001b[0;36massert_no_nans\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massert_no_nans\u001b[39m(x):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_epochs = 500\n",
    "\n",
    "while e < max_epochs:\n",
    "    train_losses = []\n",
    "    for sample, brightness in tqdm(train_loader, desc=f\"Epoch {e}\"):\n",
    "        sample = sample.to(device)\n",
    "        brightness = brightness.float().to(device)\n",
    "        prediction = model(sample)\n",
    "        loss = loss_fn(prediction, brightness)\n",
    "        train_losses.append(loss.detach().cpu().item())\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # CMAP = \"viridis\"\n",
    "        # fig, axs = plt.subplots(1,2,figsize=(12,6))\n",
    "        # target = np.squeeze(brightness.detach().cpu().numpy())\n",
    "        # pred = np.squeeze(prediction.detach().cpu().numpy())\n",
    "        # vmin = np.min(pred)\n",
    "        # vmax = np.max(pred)\n",
    "        # im = axs[0].imshow(target, vmin=vmin, vmax=vmax, cmap=CMAP)\n",
    "        # axs[0].set_title(\"Original Brightness\")\n",
    "        # axs[1].imshow(pred, vmin=vmin, vmax=vmax, cmap=CMAP)\n",
    "        # axs[1].set_title(\"ACES Brightness\")\n",
    "        # fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "\n",
    "    checkpoint[\"state_dict\"] = model.state_dict()\n",
    "    checkpoint[\"train_losses\"].append(np.mean(train_losses))\n",
    "    checkpoint[\"train_errors\"].append([np.min(train_losses), np.max(train_losses)])\n",
    "    torch.save(checkpoint, checkpoint_file_path(e))\n",
    "    print(f\"Average training loss: {checkpoint[\"train_losses\"][-1]}\")\n",
    "    e += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8451bfc-2e9c-49ca-a3b6-c0ed40df35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_range = list(range(len(checkpoint[\"train_losses\"])))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Training loss (MSE)\")\n",
    "ax.fill_between(\n",
    "    x_range,\n",
    "    [y2 for _, y2 in checkpoint[\"train_errors\"]],\n",
    "    y2=[y1 for y1, _ in checkpoint[\"train_errors\"]],\n",
    "    color=\"grey\",\n",
    "    label=\"min-max\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax.plot(x_range, checkpoint[\"train_losses\"], \"k-\", label=\"avg\")\n",
    "_ = ax.legend()\n",
    "\n",
    "print(model.weights.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf42eb-c8c0-4eb3-9e79-43e7f599c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare learning to \"canonical\" ACES coefficients from\n",
    "# https://knarkowicz.wordpress.com/2016/01/06/aces-filmic-tone-mapping-curve/\n",
    "params = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "canonical_values = dict(zip(params, [2.51, 0.03, 2.43, 0.59, 0.14]))\n",
    "\n",
    "learned_params = {p: [] for p in params}\n",
    "num_epochs = 500\n",
    "x_values = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_file_path = checkpoint_file_path(epoch)\n",
    "    if not Path(epoch_file_path).is_file():\n",
    "        continue\n",
    "    model.load_state_dict(torch.load(epoch_file_path, weights_only=False)[\"state_dict\"])\n",
    "    for p, value in zip(params, model.weights.detach().cpu().numpy(), strict=True):\n",
    "        learned_params[p].append(value)\n",
    "    x_values.append(epoch)\n",
    "    \n",
    "for param, learned_values in learned_params.items():\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(f\"Value of parameter {param}\")\n",
    "    ax.plot(x_values, learned_values, \"b-\", label=\"learned values\")\n",
    "    ax.plot(x_values, [canonical_values[param] for _ in x_values], \"k--\", label=\"canonical value\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a2458-a5c0-44ef-9a77-384396f3cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_to_use = 430\n",
    "model.load_state_dict(torch.load(checkpoint_file_path(epoch_to_use), weights_only=False)[\"state_dict\"])\n",
    "\n",
    "x_range = np.logspace(-2, 2, num=100, base=10)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.set_xlabel(\"Input value\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_ylabel(\"ACES mapping\")\n",
    "ax.plot(x_range, aces(x_range, *model.weights.detach().cpu().numpy(), key=None), \"b-\", label=\"learned\")\n",
    "ax.plot(x_range, aces(x_range, *list(canonical_values.values()), key=None), \"k--\", label=\"canonical\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b36f2a-26fe-4415-81dd-95ed7893aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Canonical\n",
    "a = 2.51\n",
    "b = 0.03\n",
    "c = 2.43\n",
    "d = 0.59\n",
    "e = 0.14\n",
    "\n",
    "# # Learned\n",
    "# a = 0.10450471\n",
    "# b = 0.06196932\n",
    "# c = 0.07076844\n",
    "# d = 2.9122088\n",
    "# e = 0.9020571\n",
    "\n",
    "for file in (data_dir / \"images\").iterdir():\n",
    "    if not file.suffix in [\".hdr\", \".exr\"]:\n",
    "        continue\n",
    "    image = dino.read_image(str(file))\n",
    "    tonemapped = aces(image, a, b, c, d, e)\n",
    "    output_path = str(data_dir / \"tonemapped\" / f\"{file.stem}_aces_a={a}_b={b}_c={c}_d={d}_e={e}.png\")\n",
    "    dino.write_image(output_path, tonemapped)\n",
    "    print(f\"Saved {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877bac4e-7cdd-4bdd-84df-ee410182306e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
