{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a06ddaaa-b2d7-4da8-a0ef-9c2894042ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import GaussianBlur\n",
    "\n",
    "import src.dino as dino\n",
    "\n",
    "\n",
    "class ACESModel(nn.Module):\n",
    "    W = 0.9\n",
    "    A = 1\n",
    "    B = 1\n",
    "    C = 1\n",
    "    D = 1\n",
    "    SCALE_NORMALIZED = True\n",
    "    KEY = 0.18\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ACESModel, self).__init__()\n",
    "        # a, b, c, d, e\n",
    "        self.weights = nn.Parameter(torch.Tensor([2.51, 0.03, 2.43, 0.59, 0.14]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply ACES tone mapper\n",
    "        x = (self.KEY / (torch.mean(x**2)**(1/2))) * x\n",
    "        a, b, c, d, e = self.weights\n",
    "        tonemapped_x = torch.squeeze((x * (a*x + b)) / (x * (c*x + d) + e))\n",
    "        # Clip results to (0, 1)\n",
    "        zeros = torch.zeros(tonemapped_x.size()).to(x.device)\n",
    "        tonemapped_x = torch.maximum(tonemapped_x, zeros)\n",
    "        ones = torch.full(tonemapped_x.size(), 1).to(x.device)\n",
    "        tonemapped_x = torch.minimum(tonemapped_x, ones)\n",
    "        # Compute brightness response\n",
    "        X, Y, Z = dino.pytorch.rgb_to_xyz(tonemapped_x)\n",
    "        L, _, _ = dino.xyz_to_lxy(X, Y, Z)\n",
    "        brightness_response = dino.pytorch.dn_brightness_model(\n",
    "            L,\n",
    "            w=self.W,\n",
    "            a=self.A,\n",
    "            b=self.B,\n",
    "            c=self.C,\n",
    "            d=self.D,\n",
    "            scale_normalized_constants=self.SCALE_NORMALIZED,\n",
    "        )\n",
    "        return brightness_response.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc07df27-7c6c-4d05-8d79-379bf657d6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00e11dc0b1b4d1ea3f0fd001c4d3292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generating training set:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training/brightnesses/CadikNight.npy already exists\n",
      "training/brightnesses/CadikDesk.npy already exists\n",
      "training/brightnesses/CadikWindow.npy already exists\n",
      "training/brightnesses/LuxoDoubleChecker.npy already exists\n",
      "training/brightnesses/HDRMark.npy already exists\n",
      "training/brightnesses/GeneralGrant.npy already exists\n",
      "training/brightnesses/DelicateArch.npy already exists\n",
      "training/brightnesses/507.npy already exists\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src import plotting\n",
    "\n",
    "\n",
    "def file_to_brightness(file_path):\n",
    "    image = dino.read_image(file_path)\n",
    "    X, Y, Z = dino.rgb_to_xyz(image)\n",
    "    L, _, _ = dino.xyz_to_lxy(X, Y, Z)\n",
    "    return dino.dn_brightness_model(\n",
    "        L,\n",
    "        w=ACESModel.W,\n",
    "        a=ACESModel.A,\n",
    "        b=ACESModel.B,\n",
    "        c=ACESModel.C,\n",
    "        d=ACESModel.D,\n",
    "        scale_normalized_constants=ACESModel.SCALE_NORMALIZED,\n",
    "    )\n",
    "\n",
    "\n",
    "# Generate brightness response images for all inputs\n",
    "data_dir = Path(\"training\")\n",
    "for file in tqdm(list((data_dir / \"images\").iterdir()), desc=\"generating training set\"):\n",
    "    export_path = str(data_dir / \"brightnesses\" / file.stem) + \".npy\"\n",
    "    if Path(export_path).exists():\n",
    "        print(f\"{export_path} already exists\")\n",
    "        brightness = np.load(export_path)\n",
    "    else:\n",
    "        brightness = file_to_brightness(str(file))\n",
    "        print(f\"Exporting {export_path}\")\n",
    "        np.save(export_path, brightness)\n",
    "    # # Visualize brightness response\n",
    "    # plotting.image_pseudocolor_plot(brightness, title=file.stem, cmap=\"Grays_r\", display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cfe97b8-aaa5-4ed9-9577-7227dd265e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda\n"
     ]
    }
   ],
   "source": [
    "class BrightnessDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, input_dir, target_dir, max_width=1000, transform=None, target_transform=None):\n",
    "        self.input_file_paths = sorted(f for f in Path(input_dir).iterdir() if f.is_file())\n",
    "        self.target_file_paths = sorted(f for f in Path(target_dir).iterdir() if f.is_file())\n",
    "\n",
    "        assert all(\n",
    "            self.input_file_paths[i].stem == self.target_file_paths[i].stem\n",
    "            for i in range(len(self.input_file_paths))\n",
    "        )\n",
    "\n",
    "        self.max_width = max_width\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_image = dino.read_image(str(self.input_file_paths[idx]))\n",
    "        input_image = dino.resize_image(\n",
    "            input_image, resize_width=self.max_width\n",
    "        )\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            \n",
    "        brightness_response = np.load(self.target_file_paths[idx])\n",
    "        brightness_response = dino.resize_image(\n",
    "            brightness_response, resize_width=self.max_width\n",
    "        )\n",
    "        if self.target_transform:\n",
    "            brightness_response = self.target_transform(brightness_response)\n",
    "\n",
    "        return input_image, brightness_response\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device is {device}\")\n",
    "model = ACESModel().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "train_set = BrightnessDataset(\"training/images\", \"training/brightnesses\")\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90495787-a115-4950-8c57-ce90dadbede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = Path(\"training/checkpoints\")\n",
    "\n",
    "def checkpoint_file_path(epoch):\n",
    "    return str(CHECKPOINT_PATH / f\"aces-epoch{epoch}.pickle\")\n",
    "\n",
    "start_epoch = 0\n",
    "if start_epoch == 0:\n",
    "    checkpoint = dict(\n",
    "        state_dict={},\n",
    "        train_losses=[],\n",
    "        train_errors=[],\n",
    "    )\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint_file_path(start_epoch), weights_only=False)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    print(f\"Loaded model from checkpoint at epoch {start_epoch}.\")\n",
    "\n",
    "e = start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e858033-db67-4e41-832d-dff2f1e1185c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbb05902c8246cd9df04ebf2d38e4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "max_epochs = 500\n",
    "\n",
    "while e < max_epochs:\n",
    "    train_losses = []\n",
    "    for sample, brightness in tqdm(train_loader, desc=f\"Epoch {e}\"):\n",
    "        sample = sample.to(device)\n",
    "        brightness = brightness.float().to(device)\n",
    "        prediction = model(sample)\n",
    "        loss = loss_fn(prediction, brightness)\n",
    "        train_losses.append(loss.detach().cpu().item())\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # CMAP = \"viridis\"\n",
    "        # fig, axs = plt.subplots(1,2,figsize=(12,6))\n",
    "        # target = np.squeeze(brightness.detach().cpu().numpy())\n",
    "        # pred = np.squeeze(prediction.detach().cpu().numpy())\n",
    "        # vmin = np.min(pred)\n",
    "        # vmax = np.max(pred)\n",
    "        # im = axs[0].imshow(target, vmin=vmin, vmax=vmax, cmap=CMAP)\n",
    "        # axs[0].set_title(\"Original Brightness\")\n",
    "        # axs[1].imshow(pred, vmin=vmin, vmax=vmax, cmap=CMAP)\n",
    "        # axs[1].set_title(\"ACES Brightness\")\n",
    "        # fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "\n",
    "    checkpoint[\"state_dict\"] = model.state_dict()\n",
    "    checkpoint[\"train_losses\"].append(np.mean(train_losses))\n",
    "    checkpoint[\"train_errors\"].append([np.min(train_losses), np.max(train_losses)])\n",
    "    torch.save(checkpoint, checkpoint_file_path(e))\n",
    "    print(f\"Average training loss: {checkpoint[\"train_losses\"][-1]}\")\n",
    "    e += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8451bfc-2e9c-49ca-a3b6-c0ed40df35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = list(range(len(checkpoint[\"train_losses\"])))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Training loss (MSE)\")\n",
    "ax.fill_between(\n",
    "    x_range,\n",
    "    [y2 for _, y2 in checkpoint[\"train_errors\"]],\n",
    "    y2=[y1 for y1, _ in checkpoint[\"train_errors\"]],\n",
    "    color=\"grey\",\n",
    "    label=\"min-max\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax.plot(x_range, checkpoint[\"train_losses\"], \"k-\", label=\"avg\")\n",
    "_ = ax.legend()\n",
    "\n",
    "print(model.weights.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf42eb-c8c0-4eb3-9e79-43e7f599c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare learning to \"canonical\" ACES coefficients from\n",
    "# https://knarkowicz.wordpress.com/2016/01/06/aces-filmic-tone-mapping-curve/\n",
    "params = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "canonical_values = dict(zip(params, [2.51, 0.03, 2.43, 0.59, 0.14]))\n",
    "\n",
    "learned_params = {p: [] for p in params}\n",
    "num_epochs = 500\n",
    "x_values = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_file_path = checkpoint_file_path(epoch)\n",
    "    if not Path(epoch_file_path).is_file():\n",
    "        continue\n",
    "    model.load_state_dict(torch.load(epoch_file_path, weights_only=False)[\"state_dict\"])\n",
    "    for p, value in zip(params, model.weights.detach().cpu().numpy(), strict=True):\n",
    "        learned_params[p].append(value)\n",
    "    x_values.append(epoch)\n",
    "    \n",
    "for param, learned_values in learned_params.items():\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(f\"Value of parameter {param}\")\n",
    "    ax.plot(x_values, learned_values, \"b-\", label=\"learned values\")\n",
    "    ax.plot(x_values, [canonical_values[param] for _ in x_values], \"k--\", label=\"canonical value\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a2458-a5c0-44ef-9a77-384396f3cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_to_use = 430\n",
    "model.load_state_dict(torch.load(checkpoint_file_path(epoch_to_use), weights_only=False)[\"state_dict\"])\n",
    "\n",
    "x_range = np.logspace(-2, 2, num=100, base=10)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.set_xlabel(\"Input value\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_ylabel(\"ACES mapping\")\n",
    "ax.plot(x_range, aces(x_range, *model.weights.detach().cpu().numpy(), key=None), \"b-\", label=\"learned\")\n",
    "ax.plot(x_range, aces(x_range, *list(canonical_values.values()), key=None), \"k--\", label=\"canonical\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b36f2a-26fe-4415-81dd-95ed7893aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Canonical\n",
    "a = 2.51\n",
    "b = 0.03\n",
    "c = 2.43\n",
    "d = 0.59\n",
    "e = 0.14\n",
    "\n",
    "# # Learned\n",
    "# a = 0.10450471\n",
    "# b = 0.06196932\n",
    "# c = 0.07076844\n",
    "# d = 2.9122088\n",
    "# e = 0.9020571\n",
    "\n",
    "for file in (data_dir / \"images\").iterdir():\n",
    "    if not file.suffix in [\".hdr\", \".exr\"]:\n",
    "        continue\n",
    "    image = dino.read_image(str(file))\n",
    "    tonemapped = aces(image, a, b, c, d, e)\n",
    "    output_path = str(data_dir / \"tonemapped\" / f\"{file.stem}_aces_a={a}_b={b}_c={c}_d={d}_e={e}.png\")\n",
    "    dino.write_image(output_path, tonemapped)\n",
    "    print(f\"Saved {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877bac4e-7cdd-4bdd-84df-ee410182306e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
