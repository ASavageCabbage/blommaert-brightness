{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a06ddaaa-b2d7-4da8-a0ef-9c2894042ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import GaussianBlur\n",
    "\n",
    "\n",
    "class ACESModel(nn.Module):\n",
    "    W = 0.9\n",
    "    A = 1\n",
    "    B = 1\n",
    "    C = 1\n",
    "    D = 1\n",
    "    SCALE_NORMALIZED = True\n",
    "    KEY = 0.18\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ACESModel, self).__init__()\n",
    "        # a, b, c, d, e\n",
    "        self.weights = nn.Parameter(torch.Tensor([2.51, 0.03, 2.43, 0.59, 0.14]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply ACES tone mapper\n",
    "        x = (self.KEY / (torch.mean(x**2)**(1/2))) * x\n",
    "        a, b, c, d, e = self.weights\n",
    "        tonemapped_x = torch.squeeze((x * (a*x + b)) / (x * (c*x + d) + e))\n",
    "        # Clip results to (0, 1)\n",
    "        zeros = torch.zeros(tonemapped_x.size()).to(x.device)\n",
    "        tonemapped_x = torch.maximum(tonemapped_x, zeros)\n",
    "        ones = torch.full(tonemapped_x.size(), 1).to(x.device)\n",
    "        tonemapped_x = torch.minimum(tonemapped_x, ones)\n",
    "        # Compute brightness response\n",
    "        X, Y, Z = dino.pytorch.rgb_to_xyz(tonemapped_x)\n",
    "        L, _, _ = dino.xyz_to_lxy(X, Y, Z)\n",
    "        brightness_response = dino.pytorch.dn_brightness_model(\n",
    "            L,\n",
    "            w=self.W,\n",
    "            a=self.A,\n",
    "            b=self.B,\n",
    "            c=self.C,\n",
    "            d=self.D,\n",
    "            scale_normalized_constants=self.SCALE_NORMALIZED,\n",
    "        )\n",
    "        return brightness_response.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc07df27-7c6c-4d05-8d79-379bf657d6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b592d97ec7408fa94279a03b2b3d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generating training set:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stated vs actual dynamic range: 13K:1 vs 972 (max: 20335, min: 20.899999618530273)\n",
      "Stated vs actual mean luminance: 303.0 vs 303\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStated vs actual dynamic range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstated_dyn_range\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_dyn_range\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (max: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mmax(L))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, min: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mmin(L),\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStated vs actual mean luminance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_L\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(L))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m brightness \u001b[38;5;241m=\u001b[39m \u001b[43mdino\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdn_brightness_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mACESModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mACESModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mACESModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mACESModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mACESModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale_normalized_constants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mACESModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSCALE_NORMALIZED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExporting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexport_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m np\u001b[38;5;241m.\u001b[39msave(export_path, brightness)\n",
      "File \u001b[0;32m/mnt/c/Users/Azimuth/Documents/GitHub/divnorm-brightness/src/dino/opencv.py:163\u001b[0m, in \u001b[0;36mdn_brightness_model\u001b[0;34m(L, gamma, cs_ratio, num_scales, w, a, b, c, d, scale_normalized_constants)\u001b[0m\n\u001b[1;32m    160\u001b[0m center_response \u001b[38;5;241m=\u001b[39m gaussian_blur(L, scales[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(scales)):\n\u001b[0;32m--> 163\u001b[0m     surround_response \u001b[38;5;241m=\u001b[39m \u001b[43mgaussian_blur\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscales\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m center_response\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m surround_response\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m/mnt/c/Users/Azimuth/Documents/GitHub/divnorm-brightness/src/dino/opencv.py:117\u001b[0m, in \u001b[0;36mgaussian_blur\u001b[0;34m(image, sigma, kernel_sigmas)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgaussian_blur\u001b[39m(\n\u001b[1;32m    113\u001b[0m     image: np\u001b[38;5;241m.\u001b[39mndarray, sigma: \u001b[38;5;28mfloat\u001b[39m, kernel_sigmas: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Ensure the kernel size is odd\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     kernel_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m kernel_sigmas \u001b[38;5;241m*\u001b[39m sigma \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m|\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 117\u001b[0m     blurred \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGaussianBlur\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43msigmaX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43msigmaY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mborderType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBORDER_DEFAULT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqueeze(blurred)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import src.dino as dino\n",
    "# from src import plotting\n",
    "\n",
    "def get_value_below(df, value):\n",
    "    df_with_value = df.where(df==value).dropna(how=\"all\").dropna(axis=1)\n",
    "    row = df_with_value.index[0]\n",
    "    column = df_with_value.columns[0]\n",
    "    return df[column][row+1]\n",
    "\n",
    "\n",
    "# Generate brightness response images for all inputs\n",
    "data_dir = Path(\"training\")\n",
    "for file in tqdm(list((data_dir / \"images\").iterdir()), desc=\"generating training set\"):\n",
    "    if not file.suffix in [\".exr\", \".hdr\"]:\n",
    "        print(f\"{file} is not an HDR image, skipping...\")\n",
    "        continue\n",
    "    export_path = str(data_dir / \"brightnesses\" / file.stem) + \".npy\"\n",
    "    if Path(export_path).exists():\n",
    "        print(f\"{export_path} already exists\")\n",
    "        brightness = np.load(export_path)\n",
    "        continue\n",
    "    data_file = str(data_dir / \"image_data\" / file.stem) + \"Data.xls\"\n",
    "    data_pd = pd.read_excel(data_file)\n",
    "    stated_dyn_range = get_value_below(data_pd, \"Dyn. Range\")\n",
    "    max_L = float(get_value_below(data_pd, \"Max.\"))\n",
    "    min_L = float(get_value_below(data_pd, \"Min.\"))\n",
    "    mean_L = float(get_value_below(data_pd, \"Mean\"))\n",
    "    image = dino.read_image(str(file))\n",
    "    L = dino.fairchild_to_relative_luminance(image)\n",
    "    # Scale to stated luminance range using z-score with sigma = (max_L - min_L) / 4\n",
    "    L = ((L - np.mean(L)) / np.std(L)) * ((max_L - min_L) / 8) + mean_L\n",
    "    actual_dyn_range = round(np.max(L) / np.min(L)) if np.min(L) > 0 else \"inf\"\n",
    "    print(f\"Stated vs actual dynamic range: {stated_dyn_range} vs {actual_dyn_range} (max: {round(np.max(L))}, min: {round(np.min(L), 1)})\")\n",
    "    print(f\"Stated vs actual mean luminance: {mean_L} vs {round(np.mean(L))}\")\n",
    "    brightness = dino.dn_brightness_model(\n",
    "        L,\n",
    "        w=ACESModel.W,\n",
    "        a=ACESModel.A,\n",
    "        b=ACESModel.B,\n",
    "        c=ACESModel.C,\n",
    "        d=ACESModel.D,\n",
    "        scale_normalized_constants=ACESModel.SCALE_NORMALIZED,\n",
    "    )\n",
    "    print(f\"Exporting {export_path}\")\n",
    "    np.save(export_path, brightness)\n",
    "    # # Visualize brightness response\n",
    "    # plotting.image_pseudocolor_plot(brightness, title=file.stem, cmap=\"Grays_r\", display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cfe97b8-aaa5-4ed9-9577-7227dd265e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda\n"
     ]
    }
   ],
   "source": [
    "class BrightnessDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, input_dir, target_dir, max_width=1000, transform=None, target_transform=None):\n",
    "        self.input_file_paths = sorted(f for f in Path(input_dir).iterdir() if f.is_file())\n",
    "        self.target_file_paths = sorted(f for f in Path(target_dir).iterdir() if f.is_file())\n",
    "\n",
    "        assert all(\n",
    "            self.input_file_paths[i].stem == self.target_file_paths[i].stem\n",
    "            for i in range(len(self.input_file_paths))\n",
    "        )\n",
    "\n",
    "        self.max_width = max_width\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_image = dino.read_image(str(self.input_file_paths[idx]))\n",
    "        input_image = dino.resize_image(\n",
    "            input_image, resize_width=self.max_width\n",
    "        )\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            \n",
    "        brightness_response = np.load(self.target_file_paths[idx])\n",
    "        brightness_response = dino.resize_image(\n",
    "            brightness_response, resize_width=self.max_width\n",
    "        )\n",
    "        if self.target_transform:\n",
    "            brightness_response = self.target_transform(brightness_response)\n",
    "\n",
    "        return input_image, brightness_response\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device is {device}\")\n",
    "model = ACESModel().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "train_set = BrightnessDataset(\"training/images\", \"training/brightnesses\")\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90495787-a115-4950-8c57-ce90dadbede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = Path(\"training/checkpoints\")\n",
    "\n",
    "def checkpoint_file_path(epoch):\n",
    "    return str(CHECKPOINT_PATH / f\"aces-epoch{epoch}.pickle\")\n",
    "\n",
    "start_epoch = 0\n",
    "if start_epoch == 0:\n",
    "    checkpoint = dict(\n",
    "        state_dict={},\n",
    "        train_losses=[],\n",
    "        train_errors=[],\n",
    "    )\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint_file_path(start_epoch), weights_only=False)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    print(f\"Loaded model from checkpoint at epoch {start_epoch}.\")\n",
    "\n",
    "e = start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e858033-db67-4e41-832d-dff2f1e1185c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbb05902c8246cd9df04ebf2d38e4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "max_epochs = 500\n",
    "\n",
    "while e < max_epochs:\n",
    "    train_losses = []\n",
    "    for sample, brightness in tqdm(train_loader, desc=f\"Epoch {e}\"):\n",
    "        sample = sample.to(device)\n",
    "        brightness = brightness.float().to(device)\n",
    "        prediction = model(sample)\n",
    "        loss = loss_fn(prediction, brightness)\n",
    "        train_losses.append(loss.detach().cpu().item())\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # CMAP = \"viridis\"\n",
    "        # fig, axs = plt.subplots(1,2,figsize=(12,6))\n",
    "        # target = np.squeeze(brightness.detach().cpu().numpy())\n",
    "        # pred = np.squeeze(prediction.detach().cpu().numpy())\n",
    "        # vmin = np.min(pred)\n",
    "        # vmax = np.max(pred)\n",
    "        # im = axs[0].imshow(target, vmin=vmin, vmax=vmax, cmap=CMAP)\n",
    "        # axs[0].set_title(\"Original Brightness\")\n",
    "        # axs[1].imshow(pred, vmin=vmin, vmax=vmax, cmap=CMAP)\n",
    "        # axs[1].set_title(\"ACES Brightness\")\n",
    "        # fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "\n",
    "    checkpoint[\"state_dict\"] = model.state_dict()\n",
    "    checkpoint[\"train_losses\"].append(np.mean(train_losses))\n",
    "    checkpoint[\"train_errors\"].append([np.min(train_losses), np.max(train_losses)])\n",
    "    torch.save(checkpoint, checkpoint_file_path(e))\n",
    "    print(f\"Average training loss: {checkpoint[\"train_losses\"][-1]}\")\n",
    "    e += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8451bfc-2e9c-49ca-a3b6-c0ed40df35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = list(range(len(checkpoint[\"train_losses\"])))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Training loss (MSE)\")\n",
    "ax.fill_between(\n",
    "    x_range,\n",
    "    [y2 for _, y2 in checkpoint[\"train_errors\"]],\n",
    "    y2=[y1 for y1, _ in checkpoint[\"train_errors\"]],\n",
    "    color=\"grey\",\n",
    "    label=\"min-max\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax.plot(x_range, checkpoint[\"train_losses\"], \"k-\", label=\"avg\")\n",
    "_ = ax.legend()\n",
    "\n",
    "print(model.weights.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf42eb-c8c0-4eb3-9e79-43e7f599c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare learning to \"canonical\" ACES coefficients from\n",
    "# https://knarkowicz.wordpress.com/2016/01/06/aces-filmic-tone-mapping-curve/\n",
    "params = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "canonical_values = dict(zip(params, [2.51, 0.03, 2.43, 0.59, 0.14]))\n",
    "\n",
    "learned_params = {p: [] for p in params}\n",
    "num_epochs = 500\n",
    "x_values = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_file_path = checkpoint_file_path(epoch)\n",
    "    if not Path(epoch_file_path).is_file():\n",
    "        continue\n",
    "    model.load_state_dict(torch.load(epoch_file_path, weights_only=False)[\"state_dict\"])\n",
    "    for p, value in zip(params, model.weights.detach().cpu().numpy(), strict=True):\n",
    "        learned_params[p].append(value)\n",
    "    x_values.append(epoch)\n",
    "    \n",
    "for param, learned_values in learned_params.items():\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(f\"Value of parameter {param}\")\n",
    "    ax.plot(x_values, learned_values, \"b-\", label=\"learned values\")\n",
    "    ax.plot(x_values, [canonical_values[param] for _ in x_values], \"k--\", label=\"canonical value\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a2458-a5c0-44ef-9a77-384396f3cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_to_use = 430\n",
    "model.load_state_dict(torch.load(checkpoint_file_path(epoch_to_use), weights_only=False)[\"state_dict\"])\n",
    "\n",
    "x_range = np.logspace(-2, 2, num=100, base=10)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.set_xlabel(\"Input value\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_ylabel(\"ACES mapping\")\n",
    "ax.plot(x_range, aces(x_range, *model.weights.detach().cpu().numpy(), key=None), \"b-\", label=\"learned\")\n",
    "ax.plot(x_range, aces(x_range, *list(canonical_values.values()), key=None), \"k--\", label=\"canonical\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b36f2a-26fe-4415-81dd-95ed7893aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Canonical\n",
    "a = 2.51\n",
    "b = 0.03\n",
    "c = 2.43\n",
    "d = 0.59\n",
    "e = 0.14\n",
    "\n",
    "# # Learned\n",
    "# a = 0.10450471\n",
    "# b = 0.06196932\n",
    "# c = 0.07076844\n",
    "# d = 2.9122088\n",
    "# e = 0.9020571\n",
    "\n",
    "for file in (data_dir / \"images\").iterdir():\n",
    "    if not file.suffix in [\".hdr\", \".exr\"]:\n",
    "        continue\n",
    "    image = dino.read_image(str(file))\n",
    "    tonemapped = aces(image, a, b, c, d, e)\n",
    "    output_path = str(data_dir / \"tonemapped\" / f\"{file.stem}_aces_a={a}_b={b}_c={c}_d={d}_e={e}.png\")\n",
    "    dino.write_image(output_path, tonemapped)\n",
    "    print(f\"Saved {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877bac4e-7cdd-4bdd-84df-ee410182306e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
